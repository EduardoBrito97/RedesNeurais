# -*- coding: utf-8 -*-
"""BTC Redes Neurais

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lMudhEKNeeE7pvvR5D1FU7IXv3GW6BAs
"""

# Commented out IPython magic to ensure Python compatibility.
#install all the required dependancy libraries
#!pip install torch

#importing the libraries
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch as torch
from torch.utils import data
from sklearn.preprocessing import StandardScaler
import io
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
# %matplotlib inline

btc = pd.read_csv('./btc.csv')
btc.head()

class LoadData(data.Dataset):
    def __init__(self, dataset_dir, window_size=7, train=True):
        btc = pd.read_csv(dataset_dir)
        data_to_use=btc['Close'].values
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data_to_use.reshape(-1, 1))
        X, y = self.window_data(scaled_data, window_size)
        if train:
            self.data  = np.array(X[:1018]).astype(np.double)
            self.label = np.array(y[:1018]).astype(np.double)
            self.dataset_size = self.data.shape[0]
        else:
            self.data = np.array(X[1018:]).astype(np.double)
            self.label = np.array(y[1018:]).astype(np.double)
            self.dataset_size = self.data.shape[0]

    def __len__(self):
        return self.dataset_size

    def __getitem__(self, idx):
        return self.data[idx], self.label[idx] 

    def window_data(self, data, window_size):
        X = []
        y = []
    
        i = 0
        while (i + window_size) <= len(data) - 1:
            X.append(data[i:i+window_size])
            y.append(data[i+window_size])
            
            i += 1
        assert len(X) ==  len(y)
        return X, y

dataset = LoadData('btc.csv', window_size=7)
dataset[0]

def load_loaders(window_size):
    train_dataset = LoadData('btc.csv', window_size=window_size)
    train_loader = data.DataLoader(dataset=train_dataset, batch_size=7, drop_last=True)
    val_dataset = LoadData('btc.csv', window_size=window_size,train=False)
    val_loader = data.DataLoader(dataset=val_dataset, batch_size=1, drop_last=True)
    return train_loader, val_loader

def load_optimizer(model,lr):
    # optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=lr)
    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    return optimizer

def eval_val_loss(model, dataset, criterion, device):
    val_loss, n_batch = 0, 0
    model.eval()
    with torch.no_grad():
        for train_data in dataset:
            x, y = [x.to(device) for x in train_data]

            y_out, _ = model(x)
            y_out_unsqueeze = y_out[:,-1,-1].unsqueeze(-1)
            value = criterion(y_out_unsqueeze, y)
            val_loss += value

            n_batch += 1
    
    return val_loss/n_batch

def train(model, device, criterion, train_loader, optimizer, epoch, writer, params):
    # set model to training mode.
    steps = 0
    model.train()
    h0 = torch.randn(params['window'], 7, params['hiden_layers']).to(device) # H da primeira célula (num_layers * num_directions, batch, hidden_size)
    c0 = torch.randn(params['window'], 7, params['hiden_layers']).to(device) # C da primeira célula (num_layers * num_directions, batch, hidden_size)
    avg_loss = 0
    with tqdm(total=len(train_loader)) as t:
        for train_data in train_loader:
            # move to GPU, if available
            x, y = [x.to(device) for x in train_data]
            # clear the gradients of all optimized variables.
            optimizer.zero_grad()

            # compute model output and loss.
            y_out, _ = model(x.double(), (h0.double(),c0.double()))
            y_out_unsqueeze = y_out[:,-1,-1].unsqueeze(-1)
            loss = criterion(y_out_unsqueeze, y)
            
            # compute gradient of the loss with respect to model parameters.
            loss.backward()
            # performs updates using calculated gradients.
            optimizer.step()
            avg_loss += loss.item()

            loss_postfix = {'loss':'{:05.6f}'.format(loss)}
            t.set_postfix(loss_postfix)
            t.update()

            steps += 1

    print("===> Epoch {} Complete: Train Loss: {}. ".format(epoch+1, avg_loss/steps), end=' ')

    # write train loss on tensorboard.
    writer.add_scalars('loss_lr'+str(params['lr'])+'_hl_'+str(params['hiden_layers']), {'window_'+str(params['window']):avg_loss/steps}, epoch+1)

def test(model, device, criterion, val_loader, epoch, writer, params):
    val_loss = eval_val_loss(model, val_loader, criterion, device)

    # print validation loss.
    print('Avg. Val Loss: {:.6f}'.format(val_loss))
    
    # write validation loss on tensorboard.
    writer.add_scalars('test_loss_lr'+str(params['lr'])+'_hl_'+str(params['hiden_layers']), {'window_'+str(params['window']):val_loss}, epoch+1)

window_sizes = [7]
lrs = [ 0.1 ]
hiden_layers = [256]
criterion = torch.nn.MSELoss()
writer = SummaryWriter()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
for w in window_sizes:
    for l in lrs:
        for h in hiden_layers:
            train_loader, val_loader = load_loaders(w)
            model = torch.nn.LSTM(1, h, w, batch_first=True).double().to(device)
            optimizer = load_optimizer(model,l)
            params = {}
            for epoch in range(70):
                params['lr'] = l
                params['hiden_layers'] = h
                params['window'] = w
                train(model, device, criterion, train_loader, optimizer, epoch, writer, params)
                
                test(model, device, criterion, val_loader, epoch, writer, params)
writer.close()

