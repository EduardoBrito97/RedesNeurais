# -*- coding: utf-8 -*-
"""BTC Redes Neurais V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11pKySRqNedRYutLe5x9hqBMsq50isgTQ
"""

# Commented out IPython magic to ensure Python compatibility.
#install all the required dependancy libraries
!pip install torch

#importing the libraries
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch as torch
from torch.utils import data
from sklearn.preprocessing import StandardScaler
import io
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
# %matplotlib inline

btc = pd.read_csv('btc.csv')
btc.head()

class LoadData(data.Dataset):
    def __init__(self, dataset_dir, window_size=7, train=True):
        btc = pd.read_csv(dataset_dir)
        data_to_use=btc['Close'].values
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data_to_use.reshape(-1, 1))
        X, y = self.window_data(scaled_data, window_size)
        if train:
            self.data  = np.array(X[:1018]).astype(np.double)
            self.label = np.array(y[:1018]).astype(np.double)
            self.dataset_size = self.data.shape[0]
        else:
            self.data = np.array(X[1018:]).astype(np.double)
            self.label = np.array(y[1018:]).astype(np.double)
            self.dataset_size = self.data.shape[0]

    def __len__(self):
        return self.dataset_size

    def __getitem__(self, idx):
        return self.data[idx], self.label[idx] 

    def window_data(self, data, window_size):
        X = []
        y = []
    
        i = 0
        while (i + window_size) <= len(data) - 1:
            X.append(data[i:i+window_size])
            y.append(data[i+window_size])
            
            i += 1
        assert len(X) ==  len(y)
        return X, y

class LSTM(torch.nn.Module):
    def __init__(self, input_size=1, hidden_feat=1, num_layers=100, seq_len=7, hidden_cell=None, output_size=1):
        super().__init__()
        self.hidden_layer_size = num_layers

        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_feat, num_layers=num_layers, batch_first=True)

        self.linear = torch.nn.Linear(seq_len, output_size)

        self.hidden_cell = hidden_cell

    def forward(self, input_seq):
        lstm_out, self.hidden_cell = self.lstm(input_seq)
        lstm_out = lstm_out.permute(0,2,1)
        predictions = self.linear(lstm_out)
        return predictions[:,:,0]

dataset = LoadData('btc.csv', window_size=7)
dataset[0]

def load_loaders(window_size):
    train_dataset = LoadData('btc.csv', window_size=window_size)
    train_loader = data.DataLoader(dataset=train_dataset, batch_size=w, drop_last=True)
    val_dataset = LoadData('btc.csv', window_size=window_size,train=False)
    val_loader = data.DataLoader(dataset=val_dataset, batch_size=1, drop_last=True)
    return train_loader, val_loader

def load_optimizer(model,lr):
    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=lr)
    #optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    #optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    return optimizer

def eval_val_loss(model, dataset, criterion, device):
    val_loss, n_batch = 0, 0
    model.eval()
    with torch.no_grad():
        for train_data in dataset:
            x, y = [x.to(device) for x in train_data]
            
            #x = x.permute(0, 2, 1)
            y_out= model(x)
            #y_out_unsqueeze = y_out[:,-1,-1].unsqueeze(-1)
            #value = criterion(y_out_unsqueeze, y)
            value = criterion(y_out, y)
            val_loss += value

            n_batch += 1
    
    return val_loss/n_batch

def train(model, device, criterion, train_loader, optimizer, epoch, writer, params):
    # set model to training mode.
    steps = 0
    model.train()
    #h0 = torch.randn(params['window'], 7, params['hiden_layers']).to(device) # H da primeira célula (num_layers * num_directions, batch, hidden_size)
    #c0 = torch.randn(params['window'], 7, params['hiden_layers']).to(device) # C da primeira célula (num_layers * num_directions, batch, hidden_size)
    avg_loss = 0
    for train_data in train_loader:
        # move to GPU, if available
        x, y = [x.to(device) for x in train_data]
        # clear the gradients of all optimized variables.
        optimizer.zero_grad()

        # compute model output and loss.
        y_out = model(x.double()) #, (h0.double(),c0.double()))
        #import pdb;pdb.set_trace()
        #y_out_unsqueeze = y_out[:,-1,-1].unsqueeze(-1)
        #loss = criterion(y_out_unsqueeze, y)
        loss = criterion(y, y_out)
            
        # compute gradient of the loss with respect to model parameters.
        loss.backward()
        # performs updates using calculated gradients.
        optimizer.step()
        avg_loss += loss.item()

        steps += 1

    # write train loss on tensorboard.
    writer.add_scalars('loss_lr'+str(params['lr'])+'_hlayers_'+str(params['h']), {'window_'+str(params['window']):avg_loss/steps}, epoch+1)
    return avg_loss/steps

def test(model, device, criterion, val_loader, epoch, writer, params):
    val_loss = eval_val_loss(model, val_loader, criterion, device)
    
    # write validation loss on tensorboard.
    writer.add_scalars('test_loss_lr'+str(params['lr'])+'_hlayers_'+str(params['h']), {'window_'+str(params['window']):val_loss}, epoch+1)
    return val_loss

window_sizes = [7, 14]
lrs = [ 0.001, 0.01]
hiden_layers = [1,7,14]
criterion = torch.nn.MSELoss()
writer = SummaryWriter()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
for w in window_sizes:
  for l in lrs:
    for h in hiden_layers:
        #print('w:'+str(w),'l:'+str(l),'h:'+str(h))
        hidden_cell = (torch.zeros(params['h'],params['window'],1).double().to(device),
                       torch.zeros(params['h'],params['window'],1).double().to(device))
        train_loader, val_loader = load_loaders(w)
        model = LSTM(input_size=1, hidden_feat=1, num_layers=h, hidden_cell=hidden_cell, seq_len=w).double().to(device)
        optimizer = load_optimizer(model,l)
        params = {}
        with tqdm(total=150) as t:
            for epoch in range(150):
                params['lr'] = l
                params['window'] = w
                params['h'] = h
                train_loss = train(model, device, criterion, train_loader, optimizer, epoch, writer, params)
                val_loss = test(model, device, criterion, val_loader, epoch, writer, params)
                loss_postfix = {'train_loss':'{:05.6f}'.format(train_loss),'val_loss':'{:05.6f}'.format(val_loss)}
                t.set_postfix(loss_postfix)
                t.update()
            
        print("===> Epochs Complete: Train Loss: {}. Val Loss {}".format(train_loss, val_loss), end=' ')
writer.close()

!zip -r /content/file.zip /content/runs
from google.colab import files
files.download("/content/file.zip")

